{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d233d-d074-447f-9433-fc114336bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a017c7-89d5-41fa-8808-61ba704a2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "T = 300\n",
    "\n",
    "beta = torch.linspace(0.0001, 0.02, T)\n",
    "\n",
    "alpha = 1. - beta\n",
    "alpha_bar = torch.cumprod(alpha, axis=0)\n",
    "alpha_bar_prev = F.pad(alpha_bar[:-1], (1, 0), value=1.0)\n",
    "\n",
    "sqrt_recip_alpha = torch.sqrt(1.0 / alpha)\n",
    "sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
    "sqrt_one_minus_alpha_bar = torch.sqrt(1. - alpha_bar)\n",
    "\n",
    "posterior_variance = beta * (1. - alpha_bar_prev) / (1. - alpha_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45775259-87fa-438a-b0c6-bda61c33984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_list(vals, t, x_shape):\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(len(t), *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alpha_bar_t = get_index_from_list(sqrt_alpha_bar, t, x_0.shape)\n",
    "    sqrt_one_minus_alpha_bar_t = get_index_from_list(\n",
    "        sqrt_one_minus_alpha_bar, t, x_0.shape\n",
    "    )\n",
    "    return (sqrt_alpha_bar_t.to(device) * x_0.to(device)\n",
    "            + sqrt_one_minus_alpha_bar_t.to(device) * noise.to(device), noise.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049cb33-d572-4302-b0b6-010c8bf14a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "            transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "            transforms.Lambda(lambda t: t * 255.),\n",
    "            transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "            transforms.ToPILImage()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "    \n",
    "    plt.imshow(reverse_transforms(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736596f5-5a32-4469-8198-a0d7a451caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ff81c-132d-412e-a68a-f142f8a0ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e815bb-5cdd-477d-b50e-47d97d1a8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size: int, image_channels: int):\n",
    "        super().__init__()\n",
    "        down_channels = (image_size, image_size * 2, image_size * 4, image_size * 8)\n",
    "        up_channels = down_channels[::-1]\n",
    "        out_dim = 1 \n",
    "        time_emb_dim = 32\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) \n",
    "                                    for i in range(len(down_channels)-1)])\n",
    "\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True)\n",
    "                                  for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], 3, out_dim)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t = self.time_mlp(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66cb8e-42cf-463c-8e83-2394d037a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef0d81-6bad-488c-8586-ddf6ce8ac5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    beta_t = get_index_from_list(\n",
    "        beta, t, x.shape\n",
    "    )\n",
    "    \n",
    "    sqrt_one_minus_alpha_bar_t = get_index_from_list(\n",
    "        sqrt_one_minus_alpha_bar, t, x.shape\n",
    "    )\n",
    "    \n",
    "    sqrt_recip_alpha_t = get_index_from_list(\n",
    "        sqrt_recip_alpha, t, x.shape\n",
    "    )\n",
    "    \n",
    "    posterior_variance_t = get_index_from_list(\n",
    "        posterior_variance, t, x.shape\n",
    "    )\n",
    "\n",
    "    model_mean = sqrt_recip_alpha_t * (\n",
    "        x - beta_t * model(x, t) / sqrt_one_minus_alpha_bar_t\n",
    "    )\n",
    "    \n",
    "    noise = torch.randn_like(x)\n",
    "    return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_last_timestep(x):\n",
    "    t = torch.full((1,), 0, device=device, dtype=torch.int64)\n",
    "    \n",
    "    beta_t = get_index_from_list(\n",
    "        beta, t, x.shape\n",
    "    )\n",
    "    \n",
    "    sqrt_one_minus_alpha_bar_t = get_index_from_list(\n",
    "        sqrt_one_minus_alpha_bar, t, x.shape\n",
    "    )\n",
    "    \n",
    "    sqrt_recip_alpha_t = get_index_from_list(\n",
    "        sqrt_recip_alpha, t, x.shape\n",
    "    )\n",
    "\n",
    "    model_mean = sqrt_recip_alpha_t * (\n",
    "        x - beta_t * model(x, t) / sqrt_one_minus_alpha_bar_t\n",
    "    )\n",
    "    \n",
    "    return model_mean\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i in reversed(range(T)):\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.int64)\n",
    "        img = sample_timestep(img, t)\n",
    "\n",
    "    img = sample_last_timestep(img)\n",
    "\n",
    "    show_tensor_image(img.detach().cpu())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa592a-26d3-43ac-a1b9-5a9013fd8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_set = torchvision.datasets.Flowers102(\n",
    "    root='.',\n",
    "    download=True,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead8143-f9a3-44fd-b3f6-c36f82f8f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    image_size=64,\n",
    "    image_channels=3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6249fb4-bfa9-48f6-af03-c230f0d36230",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, size=(BATCH_SIZE,), dtype=torch.int64, device=device)\n",
    "        loss = get_loss(model, batch[0], t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss: {loss.item()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3225e42-f21c-449a-9138-cbf1c44a5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_plot_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
